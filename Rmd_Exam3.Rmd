---
title: "Exam 3"
author: "Divya Koothan"
date: "7/9/2020"
output: word_document
---

```{r}
1. #clear the global environment 
rm(list = ls(all = TRUE))
```



```{r}
2.#library tidycensus 
library(tidycensus)
library(tidyverse)
#install census api key
census_api_key("ad36f8ac89c353658437a6bcb4af62cbc5e6a8c7")

#getting ACS data 
df <- load_variables(year = 2015,
                   "acs5")
df2 <- df <- load_variables(year = 2010,
                            "acs5")

#getting the data for both years
 gini2015<- get_acs(geography = "state",
               variables = c(estimate = c("B19083_001")),
                year = 2015)
 
#creating year variable
 gini2015 <- 
   gini2015 %>%
   mutate(year = 2015) 
 
#loading lable package     
library(data.table)
 #seting names for state 
setnames(gini2015, "NAME", "state")

 #getting 2010 data 
 gini2010<- get_acs(geography = "state",
                    variables = c(estimate = c("B19083_001")),
                    year = 2010)
 
 #creating year variable 
 gini2010 <- 
   gini2010 %>%
   mutate(year = 2010)
 
#setting names for state
setnames(gini2010, "NAME", "state")

 #appending the data frames
library(tidyverse)
inequality_panel <- bind_rows(gini2015, gini2010)

#peak at data
head(inequality_panel)
```


```{r}
3. #3
#making the panel data wide to display years
inequality_wide <-
  inequality_panel %>%
  pivot_wider(id_cols = c("GEOID","state", "year"),
              names_from = "year", 
              values_from = "estimate", 
              names_prefix = "year_")
#peak at panel
head(inequality_wide)
```


```{r}
4.#4
#pivot table so it is long again
inequality_long <- 
  inequality_wide %>%
  pivot_longer(cols = starts_with("year"),
               names_to = "year",
               names_prefix = "year_",
               values_to = "estimate",
               values_drop_na = FALSE) 
#pak at the panel
head(inequality_long)
```


```{r}
5.#5
#check to see equal observation count, listed observations then columns
dim(inequality_long)
dim(inequality_panel)
```


```{r}
6.#6
#collapsing the data
inequality_collapsed <-
  inequality_long %>%
  group_by(GEOID, state, year) %>%
  summarize(across(where(is.numeric), mean))
```


```{r}
7.
```


```{r}
8.#8
#loading library
library(WDI)
#importing GDP data from WDI from 2006-2007
gdp_current <- WDI(country = "all",
                     indicator = "NY.GDP.MKTP.CD", #GDP deflator for 2015
                     start = 2006, end= 2007, 
                     extra = FALSE, cache = NULL) 

```


```{r}
9.#deflating to the base year 2015
deflator_data <- WDI(country = "all",
                     indicator = "NY.GDP.DEFL.ZS", #GDP deflator for 2015
                     start = 2001, end= 2017, 
                     extra = FALSE, cache = NULL) 

setnames(deflator_data, "NY.GDP.DEFL.ZS", "deflator")

#subset data fram to get only US $s
usd_deflator <- subset(deflator_data, country == "United States")
#drop unnecessary variables
usd_deflator$iso2c <- NULL
usd_deflator$country <- NULL

gdp_current = left_join(x = gdp_current,
                          y = usd_deflator,
                          by = "year")
#deflation and creating new variable
gdp_current$gdp_deflated <-
  gdp_current$GDP_current/
  (gdp_current$deflator/100)

#remove excess data table
rm(deflator_data)
#peak at table 
head(gdp_current)
```
I selected 2015 as the base year, to see the inflation amount during a period of economic growth after the recession 

#10 
The three main components of a shiny app are the ui(user interface), server, and the the execution of the shinyApp() function


```{r}
11.
#loading libraries
## Pulling from PDF documents
library(pdftools) #allows to read pdf files locally and the web
library(tidyr)
library(tidytext)
library(dplyr)
library(stringr)

#loading the text from the pdf
armeniatext <- pdf_text(pdf = "PA00TNMG.pdf")
```


```{r}
12.#converting into a data frame
armeniatext <-as.data.frame(armeniatext)
```

```{r}
13.#get stop words
data("stop_words")
#tokenize 
armeniatext <-
  armeniatext %>%
  unnest_tokens(word, armeniatext)
#removing stop words
armeniatext <-
  armeniatext %>%
  anti_join(stop_words)
```


```{r}
14.#finding top 5 most used words
armeniatext%>%
  count(word, sort = TRUE) 
head(armeniatext)
```
Top 5 words and frequency
1            armenia 252
2          political 207
3         corruption 186
4         governance 185
5          democracy 132


```{r}
15.#loading library
library(rvest)
#loading Billboard Top 100 page 
hot100page <- "https://www.billboard.com/charts/hot-100"
hot100exam <- read_html(hot100page)
```


```{r}
16. #identifying the nodes in the webpage
nodes <- hot100exam %>%
  html_node("body") %>% #let's R know structure of web page
  html_children()
```


```{r}
17.#scrape for rank
rank <- hot100exam %>%
  rvest::html_nodes('body') %>%
  xml2::xml_find_all("//span[contains(@class, 
                     'chart-element__rank__number')]") %>%#gets the ranl
  rvest::html_text()

#scrape for artist
artist <- hot100exam %>%
  rvest::html_nodes('body') %>%
  xml2::xml_find_all("//span[contains(@class, 
                     'chart-element__information__artist')]") %>%#
  rvest::html_text()

#scrape for title 
title <- hot100exam %>%
  rvest::html_nodes('body') %>%
  xml2::xml_find_all("//span[contains(@class, 
                     'chart-element__information__song')]") %>%
  rvest::html_text()

#scrape for last week
#last_week <- hot100exam %>%
#  rvest::html_nodes('body') %>%
#  xml2::xml_find_all("//div[contains(@class, 
#                     'chart-element__meta')]") %>%
#  rvest::html_text()

```


Q. [link to Github repo] (https://github.com/div-koothan/Exam_3)

